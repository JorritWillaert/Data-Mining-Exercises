{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will get familiar with Anomaly Detection and Active Learning. Let us start by importing the necessary packages. While solving the exercises, feel free to import additional functionality or packages if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T06:57:43.951643Z",
     "start_time": "2020-04-28T06:57:42.620973Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell **WITHOUT** changing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(trained_classifier, X, y=None):\n",
    "    # plot variables\n",
    "    margin_size = 0.1\n",
    "    steps=100\n",
    "    \n",
    "    # ranges\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    x_range = abs(x_max - x_min)\n",
    "    y_range = abs(y_max - y_min)\n",
    "    \n",
    "    xmin, xmax = x_min - margin_size * x_range, x_max + margin_size * x_range\n",
    "    ymin, ymax = y_min - margin_size * y_range, y_max + margin_size * y_range\n",
    "    \n",
    "    # make the meshgrid based on the data \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(xmin, xmax, int(steps)),\n",
    "        np.linspace(ymin, ymax, int(steps)))\n",
    "    X_mesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # predict for the mesh\n",
    "    Z = trained_classifier.predict_proba(X_mesh)[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # plot the figure\n",
    "    plt.figure(figsize=(13, 10))\n",
    "\n",
    "    # plot the contour\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, vmin=0.0, vmax=1.0, alpha=0.8)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # plot the points\n",
    "    if y is not None:\n",
    "        for l in [0, -1, 1]:\n",
    "            ixl = np.where(y_pred == l)[0]\n",
    "            plt.scatter(X[ixl, 0], X[ixl, 1], color=plot_colors[l], marker=plot_markers[l])\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1], edgecolors=\"k\")\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.title(\"Decision boundary: red = more anomalous, blue = more normal\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Artificial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this session we generate our own dataset. Because the goal is to plot the data along with the next steps, the dataset will have only 2 features. In Anomaly Detection the majority of the data is considered as normal, while anomalies represent a small percentage of the total amount. Here, we will set the proportion of normal examples to 95% (class prior). By initializing the random seed of the *numpy* package, we make sure that everyone generates the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "class_prior = 0.95\n",
    "\n",
    "# X\n",
    "n1 = 200\n",
    "n2 = 400\n",
    "n3 = 500\n",
    "n4 = 50\n",
    "nn = n1 + n2 + n3 + n4\n",
    "na = int(nn * (1.0 - class_prior))\n",
    "X = np.vstack((\n",
    "    np.random.uniform(low=[2, 12], high=[8, 18], size=(n1, 2)),\n",
    "    np.random.multivariate_normal([7, 5], [[0.2, 0], [0, 0.2]], n2),\n",
    "    np.random.multivariate_normal([10, 11], [[0.3, 0.4], [0.4, 3]], n3),\n",
    "    np.random.multivariate_normal([12, 8], [[0.05, 0], [0, 0.05]], n4),\n",
    "    np.random.uniform(low=[0, 0], high=[15, 20], size=(na, 2)),\n",
    "))\n",
    "\n",
    "# y: when the label of an example = 0, it means we do not know its label\n",
    "Y = np.zeros(len(X), dtype=int)\n",
    "\n",
    "# let's assume we know 50 normal examples... (label = -1)\n",
    "rand_ix = np.random.choice(nn, 50, replace=False)\n",
    "Y[rand_ix] = -1\n",
    "\n",
    "# ... and 5 anomalous examples (label = 1)\n",
    "Y[-5:] = 1\n",
    "\n",
    "# colors and symbols\n",
    "PLOT_COLORS = {1: \"tab:red\", -1: \"tab:green\", 0: \"tab:blue\"}\n",
    "PLOT_MARKERS = {1: \"^\", -1: \"*\", 0: \"o\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Plot the data in a scatter plot to visually check its structure. You can set the color and the marker of the examples (points) using the given dictionaries `PLOT_COLORS` and `PLOT_MARKERS` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 1:** Can you intuitively identify the points that look like outliers/anomalies when looking at the scatter plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection: k-nearest neighbour outlier detection (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some functionality from the `sklearn` package. This package is widely used within the machine learning community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection is usually tackled as an unsupervised learning problem for two reasons.\n",
    "1. Labels are difficult to obtain in real-world anomaly detection problems. This is because anomalies do not happen a lot so you have to wait a long time to label an anomaly. When an anomaly happens, it usually corresponds to a costly problems in the real-world. For instance, a person committing credit card fraud, a machine breaking, or a leak in the water supply system of a building.\n",
    "2. Supervised learners are not great at learning good decision boundaries when there is a large class imbalance (the number of normal examples >> the number of anomalous examples) and only a handful examples to learn from.\n",
    "\n",
    "To illustrate this phenomenon, let's try to fit a supervised learning model to the data using the available labeled examples. Then, we can visually inspect how this model classifies the whole dataset and what it's decision boundary looks like.\n",
    "\n",
    "**EXERCISE:** Fit a supervised `DecisionTreeClassifier` on the labeled subset of `X` (remember the labels are stored in `Y`). Then, use the trained classifier to predict the label of every example in `X` and plot the resulting classification. Finally, plot the decision boundary of this classifier for the dataset using the `plot_decision_boundary` function. Hint: you can read about how to apply a classifier in the documentation of the `sklearn` package [https://scikit-learn.org/stable/modules/tree.html#classification]\n",
    "\n",
    "*The decision boundary plot indicates how the classifier would classify each region of the plot, indicated using the colorscale. It allows us to visually inspect whether the classifier learned a useful separation between the 2 classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 2:** In your opinion, did the classifier learn a useful decision boundary? Does this correspond to your intuition from QUESTION 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome the absence of labels, anomaly detection models exploit underlying intuitions about the anomalies and assign a real-valued score to each example that represent its degree of anomalousness (the higher the more anomalous). The model we will use in this session is called *k-nearest neighbors anomaly detection (kNN)* and it assigns to each example the distance to its k nearest neightbors as an anomaly score.\n",
    "\n",
    "Because the implementation of the model is straightforward, we will use the implemented version in `PyOD` [https://pyod.readthedocs.io/en/latest/index.html]. `PyOD` is a good library for anomaly detection containing many state-of-the-art methods.\n",
    "\n",
    "First, we need to install the library and import the model that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyod in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: joblib in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (1.1.0)\n",
      "Requirement already satisfied: matplotlib in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.13 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (1.21.6)\n",
      "Requirement already satisfied: numba>=0.35 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (0.55.1)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (1.8.0)\n",
      "Requirement already satisfied: scikit_learn>=0.20.0 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (1.0.2)\n",
      "Requirement already satisfied: six in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (1.16.0)\n",
      "Requirement already satisfied: statsmodels in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pyod) (0.13.2)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from numba>=0.35->pyod) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from numba>=0.35->pyod) (62.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from scikit_learn>=0.20.0->pyod) (3.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from matplotlib->pyod) (9.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from statsmodels->pyod) (0.5.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from statsmodels->pyod) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vincentvercruyssen/Documents/university/didactical/Data Mining - course/virtualenv/lib/python3.9/site-packages (from pandas>=0.25->statsmodels->pyod) (2022.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyod\n",
    "from pyod.models.knn import KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 3:** What is the key insight / underlying intuition behind `kNN` outlier detection? Why do greater anomaly scores correspond to more anomalous examples? Provide an answer a-priori, i.e. without running the next code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Let's now experimentally see how `kNN` outlier detection works. Fit the model to the full dataset `X`, compute the anomaly score for each example in `X` and plot the resulting scores. Finally, plot the decision boundary of this classifier for the dataset using the `plot_decision_boundary` function. Hint: because `kNN` is an *unsupervised* method, you don't need to use `Y`. You can read about how to apply `kNN` in the documentation of the `PyOD` package [https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn]. To plot the anomaly scores, you can use the `cmap` argument of the plot function in `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 4:** Can you visually identify the examples with the largest anomaly scores? How does this correspond to your intuition from QUESTION 1? How does the decision boundary compare to the one learned in QUESTION 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move on to discrete predictions. To do inference, anomaly detection models set a threshold on the anomaly scores using to the contamination factor. Examples with an anomaly score > the threshold are labeled *anomaly*, examples with an anomaly score < threshold are labeled *normal*.\n",
    "\n",
    "**EXERCISE:** Predict the discrete classes of the examples using `kNN` and starting from the anomaly scores. Plot the results. You can use the `PLOT_MARKERS` and `PLOT_COLORS` objects to plot different colors for the examples, or you can use your own imagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 5:** Do you agree with this classification? Are there points you would label differently looking at the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection: Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many unsupervised anomaly detectors, each with their own strenghts and weaknesses. A very popular detector is the *Isolation Forest* or *IForest* detector. The intuition behind this detector is as simple: anomalies are easier to isolate from the bulk of examples than the normals. The algorithm computes an *isolation score* for each example in the dataset. `IForest` works as follows:\n",
    "1. It constructs an ensemble of trees. Each tree splits the data multiple times using random axis-parallel splits. This means that in each node of the tree, the algorithm picks a random feature and a random split value for that feature (based on the range of the remaining examples in the node). The data are split into to groups and the process repeats itself until each node contains only a single example.\n",
    "2. For each example, it computes the path length between the root node and the leaf node of that example. In other words, it counts the number of splits needed to isolate the example.\n",
    "3. It computes the average path length of the example across the ensemble. This is the final anomaly score for that example.\n",
    "\n",
    "**EXERCISE:** Apply the `IForest` model to the dataset `X`. Then, plot the resulting decision boundary using the `plot_decision_boundary` function. You can also plot the anomaly scores if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 6:** How does `IForest` differ from `KNN`? Can you explain some of the differences knowing the internal procedures of both algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Try tuning the hyperparameters of `IForest` and `KNN` to get better results. Reason about how these hyperparameters affect the resulting classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the biases of different anomaly detectors, we will slightly restructure the dataset we have been working with. We reduce the size of the upper-left cluster and the small rightmost cluster. We also increase the size of the middle 2 clusters. Finally, we drop the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "n1 = 100 # <-- 200\n",
    "n2 = 500 # <-- 400\n",
    "n3 = 600 # <-- 500\n",
    "n4 = 20 # <-- 50\n",
    "nn = n1 + n2 + n3 + n4\n",
    "na = int(nn * (1.0 - class_prior))\n",
    "X = np.vstack((\n",
    "    np.random.uniform(low=[2, 12], high=[8, 18], size=(n1, 2)),\n",
    "    np.random.multivariate_normal([7, 5], [[0.2, 0], [0, 0.2]], n2),\n",
    "    np.random.multivariate_normal([10, 11], [[0.3, 0.4], [0.4, 3]], n3),\n",
    "    np.random.multivariate_normal([12, 8], [[0.05, 0], [0, 0.05]], n4),\n",
    "    np.random.uniform(low=[0, 0], high=[15, 20], size=(na, 2)),\n",
    "))\n",
    "\n",
    "# ... all labels are unknown\n",
    "Y = np.zeros(len(X), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Plot the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still clearly see 4 clusters in the dataset. Ideally, the anomaly detector can also discover this structure and find anomalies relative to the clusters. Let's put this theory to the test.\n",
    "\n",
    "**EXERCISE:** Fit the `KNN` and `IForest` models to the updated dataset `X`. Plot for both algorithms the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 7:** How do `KNN` and `IForest` deal with the change in data density? Did you expect these changes? Why/why not? Do you think that the dataset has fundamentally changed and that the changes in decision boundaries properly reflect these changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that `IForest` has trouble discovering the relevant structure in the dataset. That is because the splits in the nodes are heavily skewed by the data density, making it difficult for smaller clusters to be identified as normal. However, we can fix this! One solution would be to explicitly extract the clusters from the data and apply `IForest` to each cluster separately. This forces the algorithm to treat each cluster as a separate \"context\" and only look for anomalies within a certain context.\n",
    "\n",
    "Let's construct this *augmented* dataset by adding a new feature that specifies the cluster to which each example belongs, i.e., the context (anomalies are assigned to random contexts). Run the following code to construct the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cluster\n",
    "X_cluster = np.zeros(len(X), dtype=float)\n",
    "X_cluster[n1:n1+n2] = 1.0\n",
    "X_cluster[n1+n2:n1+n2+n3] = 2.0\n",
    "X_cluster[n1+n2+n3:n1+n2+n3+n4] = 3.0\n",
    "X_cluster[n1+n2+n3+n4:] = np.random.choice([0.0, 1.0, 2.0, 3.0], na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADVANCED EXERCISE:** Train a separate `IForest` model for each cluster, combine the predictions of the different models. Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 8:** Does the resulting classification look beter than in QUESTION 7? Do you think we solved the problem now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
